{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data wrangling \n",
    "import pandas as pd \n",
    "\n",
    "# Deep learning \n",
    "import tensorflow as tf\n",
    "import keras \n",
    "\n",
    "# Array math \n",
    "import numpy as np\n",
    "\n",
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Mean scaler \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Loading the memory profile extension\n",
    "from memory_profiler import profile\n",
    "import sys \n",
    "\n",
    "# Ploting \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Iteration tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Metrics \n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('data/train.csv')\n",
    "\n",
    "print(f\"Shape of the data: {d.shape}\")\n",
    "print(d.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the size of the object in memory\n",
    "print(f\"The object takes: {sys.getsizeof(d) / 10**6} MB in memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in memory uses ~484MB of RAM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_vars(d):\n",
    "    \"\"\"\n",
    "    Creates the datetime variables\n",
    "    \"\"\"\n",
    "    # Infering the day of the week from pickup_datetime\n",
    "    d['pickup_datetime'] = pd.to_datetime(d['pickup_datetime'])\n",
    "    d['pickup_dayofweek'] = d['pickup_datetime'].dt.dayofweek\n",
    "\n",
    "    # Infering the hour of the day from pickup_datetime\n",
    "    d['pickup_hour'] = d['pickup_datetime'].dt.hour\n",
    "\n",
    "    # Creating a new variable for the day of the year\n",
    "    d['pickup_dayofyear'] = d['pickup_datetime'].dt.dayofyear\n",
    "\n",
    "    # Ensuring a monotonic relationship between pickup_hour and pickup_dayofyear\n",
    "    d['pickup_hour_sin'] = np.sin(2 * np.pi * d['pickup_hour']/23.0)\n",
    "    d['pickup_hour_cos'] = np.cos(2 * np.pi * d['pickup_hour']/23.0)\n",
    "\n",
    "    d['pickup_dayofyear_sin'] = np.sin(2 * np.pi * d['pickup_dayofyear']/365.0)\n",
    "    d['pickup_dayofyear_cos'] = np.cos(2 * np.pi * d['pickup_dayofyear']/365.0)\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy variables\n",
    "\n",
    "The features that will be one-hot encoded: \n",
    "\n",
    "* store_and_fwd_flag\n",
    "* vendor_id \n",
    "* pickup_dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the dummy var list \n",
    "dummy_features = [\n",
    "    'vendor_id',\n",
    "    'store_and_fwd_flag',\n",
    "    'pickup_dayofweek'\n",
    "]\n",
    "\n",
    "# Defining the function for dummy creation \n",
    "def create_dummy(df, dummy_var_list):\n",
    "    # Placeholder for the dummy variables\n",
    "    added_features = []\n",
    "    for var in dummy_var_list:\n",
    "        dummy = pd.get_dummies(df[var], prefix=var, drop_first=True)\n",
    "        \n",
    "        # Adding the new features to list \n",
    "        added_features.extend(dummy.columns)\n",
    "\n",
    "        # Adding the dummy variables to the dataframe\n",
    "        df = pd.concat([df, dummy], axis=1)\n",
    "        df.drop(var, axis=1, inplace=True)\n",
    "\n",
    "    # Returning the dataframe \n",
    "    return df, added_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance of travel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function for distance calculation\n",
    "def distance_calculation(df):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two points on the earth's surface.\n",
    "\n",
    "    The distance is in meters\n",
    "    \"\"\"\n",
    "    R = 6373.0\n",
    "\n",
    "    lat1 = np.radians(df['pickup_latitude'])\n",
    "    lon1 = np.radians(df['pickup_longitude'])\n",
    "    lat2 = np.radians(df['dropoff_latitude'])\n",
    "    lon2 = np.radians(df['dropoff_longitude'])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "\n",
    "    # Saving the distance to the dataframe\n",
    "    df['distance'] = distance * 1000 # Converting to meters\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final feature list and the ft engineering pipeline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the final feature list \n",
    "numeric_features = [\n",
    "    'distance',\n",
    "    'passenger_count', \n",
    "    'pickup_hour_sin',\n",
    "    'pickup_hour_cos',\n",
    "    'pickup_dayofyear_sin',\n",
    "    'pickup_dayofyear_cos',\n",
    "]\n",
    "\n",
    "# Defining the target variable\n",
    "target = 'trip_duration'\n",
    "\n",
    "# Defining the ft engineering pipeline \n",
    "def ft_engineering_pipeline(\n",
    "    df, \n",
    "    numeric_features, \n",
    "    dummy_features,\n",
    "    target):\n",
    "    \"\"\"\n",
    "    Applies the feature engineering pipeline to the data\n",
    "    \"\"\"\n",
    "    # Creating the date variables\n",
    "    df = create_date_vars(df)\n",
    "\n",
    "    # Creating the dummy variables\n",
    "    df, new_features = create_dummy(df, dummy_features)\n",
    "\n",
    "    # Appending the distance\n",
    "    df = distance_calculation(df) \n",
    "\n",
    "    # Appending the new features to the numeric features\n",
    "    final_features = numeric_features + new_features\n",
    "\n",
    "    # Creating the x matrix \n",
    "    x = df[final_features].values\n",
    "\n",
    "    # Creating the y vector\n",
    "    y = df[target].values\n",
    "\n",
    "    # Mean max scaling the y matrix \n",
    "    y = y.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    y = scaler.fit_transform(y)\n",
    "\n",
    "    # Returning the x and y matrices\n",
    "    return x, y, final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the input for model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, features = ft_engineering_pipeline(d, numeric_features, dummy_features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of x: {x.shape} | Shape of y: {y.shape}\")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the batch size and number of epochs \n",
    "batch_size = 512\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model function \n",
    "def train(x, y, epochs: int = 10, batch_size: int = 128): \n",
    "    # Defining a simple feed forward network \n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu, input_shape=(x.shape[1],)),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mean_squared_error']\n",
    "    )\n",
    "\n",
    "    # Fitting the model\n",
    "    history = model.fit(x, y, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    # Returning the model\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with RAM usage \n",
    "model, history = train(x, y, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using an iterator to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an iterator over the csv \n",
    "d = pd.read_csv('data/train.csv', chunksize=batch_size, iterator=True)\n",
    "\n",
    "# Getting the size of the object in memory\n",
    "print(f\"The object takes: {sys.getsizeof(d) / 10**6} MB in memory\")\n",
    "\n",
    "# Iterating over the chunks to get the final number of batches \n",
    "n_batches = 0\n",
    "\n",
    "# Creating the min-max constants for y\n",
    "min = np.inf\n",
    "max = -np.inf\n",
    "\n",
    "# Creating a dictionary for the categorical features that will store unique values\n",
    "cat_dict = {}\n",
    "\n",
    "for chunk in tqdm(d):\n",
    "    # Searching for the min and max values of y\n",
    "    if chunk['trip_duration'].min() < min:\n",
    "        min = chunk['trip_duration'].min()\n",
    "    if chunk['trip_duration'].max() > max:\n",
    "        max = chunk['trip_duration'].max()\n",
    "\n",
    "    # Creating the date variables\n",
    "    chunk = create_date_vars(chunk)\n",
    "\n",
    "    # Iterating over the cate features and getting the unique values\n",
    "    for cat in dummy_features:\n",
    "        if cat not in cat_dict.keys():\n",
    "            cat_dict[cat] = list(set(chunk[cat].unique()))\n",
    "        else:\n",
    "            # Extracting the current unique values\n",
    "            current_unique = list(set(chunk[cat].unique()))\n",
    "\n",
    "            # Getting the new unique values\n",
    "            new_unique = list(set(current_unique) - set(cat_dict[cat]))\n",
    "\n",
    "            # Adding the new unique values to the dictionary\n",
    "            cat_dict[cat].extend(new_unique)\n",
    "\n",
    "    n_batches += 1\n",
    "\n",
    "print(f\"The number of batches is: {n_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a one hot encoder for the categorical features\n",
    "cat_encoders = {}\n",
    "for cat_feature in cat_dict.keys():\n",
    "    # Creating the one hot encoder\n",
    "    one_hot = OneHotEncoder(categories='auto')\n",
    "\n",
    "    # Fitting the one hot encoder\n",
    "    one_hot.fit(np.array(cat_dict[cat_feature]).reshape(-1, 1))\n",
    "\n",
    "    # Saving the encoder to the dictionary\n",
    "    cat_encoders[cat_feature] = one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the final feature list \n",
    "final_features = [\n",
    "    'distance',\n",
    "    'passenger_count', \n",
    "    'pickup_hour_sin',\n",
    "    'pickup_hour_cos',\n",
    "    'pickup_dayofyear_sin',\n",
    "    'pickup_dayofyear_cos',\n",
    "]\n",
    "\n",
    "# Adding the final features from the one hot encoders\n",
    "for cat_feature in cat_encoders.keys():\n",
    "    # Extracting all original values\n",
    "    original_values = cat_dict[cat_feature]\n",
    "\n",
    "    # Getting the transformed values\n",
    "    out_values = cat_encoders[cat_feature].get_feature_names_out().tolist()\n",
    "\n",
    "    # Adding the names of the feature as a prefix\n",
    "    new_features = [f\"{cat_feature}_{value.split('_')[-1]}\" for value in out_values]\n",
    "\n",
    "    # Adding the new features to the list\n",
    "    final_features.extend(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a custom label encoding function \n",
    "def custom_transform(enc, x, prefix):\n",
    "    \"\"\"\n",
    "    Applies a custom transformation to the data\n",
    "    \"\"\"\n",
    "    # Transforming the data\n",
    "    out = enc.transform(x.reshape(-1, 1))\n",
    "\n",
    "    # Getting the transformed values\n",
    "    out_values = enc.get_feature_names_out().tolist()\n",
    "\n",
    "    # Adding the names of the feature as a prefix\n",
    "    out_values = [f\"{prefix}_{value.split('_')[-1]}\" for value in out_values]\n",
    "\n",
    "    # Converting to a dataframe\n",
    "    out = pd.DataFrame(out.toarray(), columns=out_values)\n",
    "\n",
    "    # Changing the datatype to uint8\n",
    "    out = out.astype('uint8')\n",
    "\n",
    "    # Returning the transformed data\n",
    "    return out\n",
    "\n",
    "# Defining a list of dummy features \n",
    "dummy_features = [\n",
    "    'vendor_id',\n",
    "    'store_and_fwd_flag',\n",
    "    'pickup_dayofweek',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the class for the batches creation \n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(\n",
    "        self, \n",
    "        csv_generator,\n",
    "        n_batches\n",
    "        ):\n",
    "        self.csv_generator = csv_generator\n",
    "        self.n_batches = n_batches\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        The total length of the iterator\n",
    "        \"\"\"\n",
    "        return self.n_batches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        The batch generator \n",
    "        \"\"\"\n",
    "        # Getting the batch\n",
    "        chunk = self.csv_generator.get_chunk()\n",
    "\n",
    "        # Reseting the index\n",
    "        chunk = chunk.reset_index(drop=True)\n",
    "\n",
    "        # Creating the date variables\n",
    "        chunk = create_date_vars(chunk)\n",
    "\n",
    "        # Creating the distance variable\n",
    "        chunk = distance_calculation(chunk) \n",
    "\n",
    "        # Creating the dummy variables\n",
    "        for cat_feature in dummy_features:\n",
    "            # Extracting the values\n",
    "            x = chunk[cat_feature].values\n",
    "\n",
    "            # Transforming the data\n",
    "            out = custom_transform(cat_encoders[cat_feature], x, cat_feature)\n",
    "\n",
    "            # Concatenating the data\n",
    "            chunk = pd.concat([chunk, out], axis=1)\n",
    "\n",
    "            # Deleting the out, x from memory\n",
    "            del out, x\n",
    "\n",
    "        # Getting the target var \n",
    "        y = chunk[target].values\n",
    "\n",
    "        # Min max transforming the y \n",
    "        y = (y - min) / (max - min)\n",
    "\n",
    "        # If any of the final features are missing we fill them with 0\n",
    "        missing_cols = set(final_features) - set(chunk.columns)\n",
    "        for c in missing_cols:\n",
    "            chunk[c] = 0\n",
    "\n",
    "        # Extracting the final features\n",
    "        x = chunk[final_features].values\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(\n",
    "    path_to_csv,\n",
    "    n_batches,\n",
    "    final_features,\n",
    "    epochs: int = 10,\n",
    "    batch_size: int = 128\n",
    "    ): \n",
    "    # Defining a simple feed forward network \n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu, input_shape=(len(final_features),)),\n",
    "        keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mean_squared_error']\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Creating the generator\n",
    "        d = pd.read_csv(path_to_csv, chunksize=batch_size, iterator=True)\n",
    "        generator = DataGenerator(d, n_batches)\n",
    "\n",
    "        # Fitting the model\n",
    "        model.fit(generator, epochs=1, verbose=1, batch_size=batch_size)\n",
    "\n",
    "    # Returning the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model \n",
    "model = train_generator(\n",
    "    path_to_csv='data/train.csv',\n",
    "    n_batches=n_batches,\n",
    "    final_features=final_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cuda_gpu_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1843498fe16f59076ae8a859587e2a2b45b21ed510245a5df4c4791774183e51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
